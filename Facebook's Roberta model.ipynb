{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Roberta.\n",
    "\n",
    "This is a quick test to see how Roberta performs on sentences it has not seen before and to check if it believes the sentences are contradicting each other, are neutral or are in contextual agreement or entailment. The model was already fine-tuned by the team at Facebook for the MNLI dataset task. I was curious to see how it did on a small sample of sentences it had not seen before.\n",
    "\n",
    "The model has three possible answers with respect to each pair of sentences it is asked to classify.  \n",
    "\n",
    "1. The sentence pairs contradict each other.\n",
    "2. They are neutral with respect to each other.\n",
    "3. They are in agreement and follow on logically from the first to the second sentence,(entailment).\n",
    "\n",
    "Results are below.\n",
    "\n",
    "##### Aug 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/brianfarrell/.cache/torch/hub/pytorch_fairseq_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.mnli.tar.gz from cache at /Users/brianfarrell/.cache/torch/pytorch_fairseq/7685ba8546f9a5ce1a00c7a6d7d44f7e748d22681172f0f391c3d48f487c801c.74e37d47306b3cc51c5f8d335022a392c29f1906c8cd9e9cd3446d7422cf55d8\n",
      "| dictionary: 50264 types\n"
     ]
    }
   ],
   "source": [
    "#Dependency\n",
    "import torch\n",
    "#Get model from Facebook torch hub\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello_roberta():     \n",
    "    #Three sentence pairs to test the model on.\n",
    "    s1=[('Jamie is CEO of a bank as of August 2019.'),('The client asked me to do it.'),('The client wanted me to buy it on the open and call her stright away.')] \n",
    "    s2=[('Jamie was fired from the bank in July 2019.'),('I didnt get permission from the client.'),('Once I bought it for the client I called the client immediately.')] \n",
    "    for i in range(3): \n",
    "        label= {0:'Contradiction',1:'Neutral',2:'Entailment'}\n",
    "        pred=[]\n",
    "        tokens = roberta.encode(s1[i], s2[i])\n",
    "        prediction = roberta.predict('mnli', tokens).argmax().item()     \n",
    "        for k,v in label.items():\n",
    "            if prediction == k:\n",
    "                pred.append(v)\n",
    "                print(\"\")            \n",
    "                print(s1[i],s2[i]) \n",
    "                print(\"Roberta's prediction:\",pred)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jamie is CEO of a bank as of August 2019. Jamie was fired from the bank in July 2019.\n",
      "Roberta's prediction: ['Contradiction']\n",
      "\n",
      "The client asked me to do it. I didnt get permission from the client.\n",
      "Roberta's prediction: ['Contradiction']\n",
      "\n",
      "The client wanted me to buy it on the open and call her stright away. Once I bought it for the client I called the client immediately.\n",
      "Roberta's prediction: ['Entailment']\n"
     ]
    }
   ],
   "source": [
    "#Call the model \n",
    "hello_roberta()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
