 
Testing Facebook's new ROBERTA model. 

ROBERTA is Facebook's twist on Google's BERT model.

Both models are now the leaders in natural language processing/understanding tasks. 

In essence both machine learning models are the best in class at reading and understanding language. From that basis they can be fine-tuned to carry out multiple tasks including reading emails, social media content or any content input to browsers, etc.

In the attached file I tested the ROBERTA model on some sentence pairs it had not seen before. 

Typically I use machine learning models like these to help classify content that might be helpful to risk professionals within the financial services domain, thus cutting down on the amount of time humans need to spend sifting through vast quantities of content that usually is not of interest and was usually retrieved using blunt keyword search software.

Non data-centric companies will use keyword search tools to flag electronic communications that are of interest. In many cases the same companies will not regard language data or unstructured data as having much value which is in complete contrast to data-centric companies who live because of data in all its forms. 

These keyword search tools tend to produce a lot of useless flagged content. Then employees of the same organisations will monitor and sift through this content. NLP models like ROBERTA are now at the stage where many of the blunt keyword search tools used by non-data centric companies are really antiquated and an inefficient use of employees time. 

It feels like just a matter of time before the tools developed by Amazon, Facebook, Google and Microsoft are embraced in one form or another by non-data centric companies.
 
